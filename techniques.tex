Like many other tasks in NLP, this problem is challenging because there exists no training data. This makes it difficult to build certain deep learning models. Annotating data in text form can be costly, difficult, and time consuming, making the evaluation of many NLP tasks strenuous. Very few large scale training sets are available for research or industrial NLP tasks~\cite{DBLP:journals/corr/abs-1803-11175}. Even for humans, labelling, extracting information, and categorizing text can be a challenge. For a computer, processing human languages can be difficult for a number of reasons. Real-world text data can be incomplete, inconsistent, illogical, contain various character encodings, or have special characters. Human language is complex and can be filled with nuance. Word order and context can completely change the intrinsic meaning of a sentence. In natural language, words can have different meanings based on syntax or semantics. For example, the word book can be either a noun or a verb based on context, such as ``I read a book,'' or ``I booked a vacation.'' In even more challenging scenarios, sentences can have more than one reasonable interpretation. ``I saw the dog on the beach with my binoculars'' could have an ambiguous meaning. It could mean to infer that the subject saw the dog through the subject’s binoculars, or alternatively that the dog on the beach was in possession of the subject’s binoculars. In order to understand the intended meaning, it would be necessary to have the full context of the sentence within a paragraph or larger corpus. 

In addition to the challenge posed by the lack of annotated training data in this research problem, there exists many other complications with the job offer corpus compiled by X28. The largest problem is that a large number of job offers are not actually environmentally related. Although they are meant to be selected for their environmental nature, the corpus includes non-green job offers for positions such as quality manager, watch makers, building managers, and pharmaceutical technicians. Furthermore, an-insignificant number of offers contains little to no text, or are incomplete or incomprehensible. The text throughout the corpus also are filled with special characters and HTML text formatting elements, making it difficult to read. Finally, the corpus contains text written in English, French, Italian, and German. 
Given the objectives of the research and the quality of the data provided, the research tasks are split into two tasks. The first task specified is to design a classifier that can split the job offers into ``environmentally related” and ``non environmentally related” categories, and the second task specified is to extract the specified activities and skills in each job offer, and match them to the standard activity and skill database compiled from O*Net. A methodology for each task is needed to be designed to produce results with little to no training data, as none are available at the time of this research.

To address the first challenge of building an “environmentally related” and “non environmentally related” classifier, an artificial neural network is built using the Python package Keras. The benefits of using neural networks or other deep learning models for NLP tasks is that learning can be done unsupervised, and that these models can also handle the recursivity of natural language, which are composed of complex word and sentence structures~\cite{10.5555/3208611}. With regards to the second task, extracting the specified activities from each job offer, an approach utilizing semantic sentence similarity is chosen. Within NLP, semantic similarity refers to a task that outputs a score of the relationship between two different texts. The most common way to accomplish this is to use a model that encodes the individual sentences in the text to obtain their embeddings, and then computes the numerical distance between the embeddings, most commonly with a cosine distance. This type of approach is again chosen as a practical option because it can be used without any training data.

Prior to implementing either of these strategies, the first steps taken are to clean the job offers corpus and to remove the HTML tags and special characters. A function is written to only allow UTF-8 encoded alphanumeric characters. Next, the text is split on periods into individual sentences. This process in NLP is called tokenization. Tokenization is a way of splitting larger pieces of text into smaller units. This is done so that textual analysis can be done on a smaller level, allowing for faster processing and more precise identification and location of certain information. Following tokenization, the Python package EasyNMT was used to translate the corpus entirely into English for uniformity and for the ability to manually inspect the results of the analyses. 
